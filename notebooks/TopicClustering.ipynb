{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# SQL packages\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatizePhrase(phrase):\n",
    "    \"\"\" \n",
    "    Lemmatize each word in a pharse\n",
    "    \"\"\"\n",
    "    words = phrase.lower().split()\n",
    "    return ' '.join(WordNetLemmatizer().lemmatize(word) for word in words)\n",
    "\n",
    "## Communicate with SQL database and get recipe tags and description\n",
    "def getRecipeDescriptionAndTags():\n",
    "    username = 'nemo'\n",
    "    pswd = 'eatsalot'\n",
    "    dbname = 'recipes_db'\n",
    "    engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "    print engine.url\n",
    "    \n",
    "    con = None\n",
    "    con = psycopg2.connect(database = dbname, user = username, host='localhost', password=pswd)\n",
    "    cur = con.cursor()\n",
    "    cur.execute(\"SELECT description,tags,id FROM recipes_db;\")\n",
    "    dfx = pd.DataFrame(cur.fetchall(), columns = ['description', 'tags', 'id'])\n",
    "    return dfx  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use tags as topics you want to model! (maybe, I still don't understand what I want to really do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://nemo:eatsalot@localhost/recipes_db\n"
     ]
    }
   ],
   "source": [
    "df = getRecipeDescriptionAndTags();\n",
    "df['tags'] = df.tags.str.rsplit(\",\")\n",
    "tempTags = list(itertools.chain.from_iterable(list(df.tags)))\n",
    "tags = list(set(tempTags))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tags are ok. But doesn't really describe food. Like, spicy, cool, comfort food, etc. maybe the description will have better info!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create a corpus with words from description\n",
    "# Remove shitty unicode characters\n",
    "#df['description'] = df['description'].str.decode('utf-8').str('utf-8')#replace('[\\xc2\\x99]', ' ')\n",
    "df['description'] = df['description'].apply(lambda x: unicode(x, errors='ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## tokenize words. Basically split words from sentences. To remove punctuations I used RegexpTokenizer!\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df['tokens'] = df['description'].apply(lambda x:tokenizer.tokenize(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'A',\n",
       " u'true',\n",
       " u'demi',\n",
       " u'glace',\n",
       " u'the',\n",
       " u'luscious',\n",
       " u'sauce',\n",
       " u'made',\n",
       " u'by',\n",
       " u'reducing',\n",
       " u'homemade',\n",
       " u'stock',\n",
       " u'and',\n",
       " u'red',\n",
       " u'wine',\n",
       " u'to',\n",
       " u'a',\n",
       " u'rich',\n",
       " u'concentration',\n",
       " u'needs',\n",
       " u'to',\n",
       " u'simmer',\n",
       " u'for',\n",
       " u'several',\n",
       " u'hours',\n",
       " u'making',\n",
       " u'it',\n",
       " u'the',\n",
       " u'perfect',\n",
       " u'activity',\n",
       " u'for',\n",
       " u'a',\n",
       " u'lazy',\n",
       " u'winter',\n",
       " u'Sunday']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now remove stop words\n",
    "df['tokens'] = df['tokens'].apply(lambda x:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
