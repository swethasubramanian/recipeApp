{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the optimization problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-banner\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"5e6a48b1-ffbc-4bb0-86a6-e4f277047196\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\") {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  var js_urls = ['https://cdn.pydata.org/bokeh/release/bokeh-0.11.1.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.11.1.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-compiler-0.11.1.min.js'];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      Bokeh.$(\"#5e6a48b1-ffbc-4bb0-86a6-e4f277047196\").text(\"BokehJS successfully loaded\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.11.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.11.1.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.11.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.11.1.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i](window.Bokeh);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk, pickle, re, itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from bokeh.charts import Bar, Histogram, Scatter, BoxPlot, output_notebook, show, output_file\n",
    "from bokeh.charts.attributes import ColorAttr, CatAttr\n",
    "import bokeh.plotting\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# SQL packages\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import nltk\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "\n",
    "output_notebook()\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider only vegetarian/vegan recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getRecipesFormTags(tags):\n",
    "    username = 'nemo'\n",
    "    pswd = 'eatsalot'\n",
    "    dbname = 'catrecipes_db'\n",
    "    engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "    print engine.url\n",
    "    \n",
    "    con = None\n",
    "    con = psycopg2.connect(database = dbname, user = username, host='localhost', password=pswd)\n",
    "    cur = con.cursor()\n",
    "    df = pd.DataFrame(columns = ['tag', 'recipe_ids', 'count', 'tag2'])\n",
    "    for tag in tags:\n",
    "        cur.execute(\n",
    "            \"SELECT * FROM tags_recipe_ids WHERE tags_recipe_ids.index = %s;\", (tag,))\n",
    "        dfx = pd.DataFrame(cur.fetchall(), columns = ['tag', 'recipe_ids', 'count', 'tag2'])\n",
    "        df = pd.concat([df, dfx], ignore_index = True)\n",
    "        #df['recipe_ids'] = df['recipe_ids'].str.split(',')\n",
    "        #recipesList = list(set(sum([recipe_id for recipe_id in df.recipe_ids.str.split(',')], [])))\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRecipesForVeggies(vegs):\n",
    "    username = 'nemo'\n",
    "    pswd = 'eatsalot'\n",
    "    dbname = 'veggie_db'\n",
    "    engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "    print engine.url\n",
    "    \n",
    "    \n",
    "    con = None\n",
    "    con = psycopg2.connect(database = dbname, user = username, host='localhost', password=pswd)\n",
    "    cur = con.cursor()\n",
    "    df = pd.DataFrame(columns = ['veggie', 'shelf_life', 'recipes', 'recipe_count'])\n",
    "    for veggie in vegs:\n",
    "        cur.execute(\n",
    "            \"SELECT * FROM veggie_info_db WHERE veggie_info_db.index = %s;\", (veggie,))\n",
    "        dfx = pd.DataFrame(cur.fetchall(), columns = ['veggie', 'shelf_life', 'recipes', 'recipe_count'])\n",
    "        df = pd.concat([df, dfx], ignore_index = True)\n",
    "        #df['recipe_ids'] = df['recipe_ids'].str.split(',')\n",
    "        #recipesList = list(set(sum([recipe_id for recipe_id in df.recipe_ids.str.split(',')], [])))\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getRecipeIngredientsForVeggies(selectedRecipesList, veggies1):\n",
    "    username = 'nemo'\n",
    "    pswd = 'eatsalot'\n",
    "    dbname = \"recipe_ingredients_db\"\n",
    "    vegs = veggies1+['num_servings', 'index']\n",
    "    \n",
    "    \n",
    "    engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "    print engine.url\n",
    "    con = None\n",
    "    con = psycopg2.connect(database = dbname, user = username, host='localhost', password=pswd)\n",
    "    cur = con.cursor()\n",
    "    SQL_query = \"SELECT \" +','.join(vegs)+  \" FROM recipe_ingredients_quant WHERE index IN %s\"\n",
    "    cur.execute(SQL_query, (tuple(selectedRecipesList),))\n",
    "    dfx = pd.DataFrame(cur.fetchall(), columns = vegs)\n",
    "    # query:\n",
    "    #sql_query = \"\"\"\n",
    "    #SELECT * FROM recipe_ingredients_quant;\n",
    "    #\"\"\"\n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRecipeImageURLs(selDf):\n",
    "    username = 'nemo'\n",
    "    pswd = 'eatsalot'\n",
    "    dbname = 'recipes_db'\n",
    "    engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "    con = None\n",
    "    con = psycopg2.connect(database = dbname, user = username, host='localhost', password=pswd)\n",
    "    cur = con.cursor()\n",
    "    SQL_query = \"SELECT image_url, id FROM recipes_db WHERE id IN %s\"\n",
    "    cur.execute(SQL_query, (tuple(list(set(selDf['index']))),))\n",
    "    dfx = pd.DataFrame(cur.fetchall(), columns = ['image_url', 'id'])\n",
    "    return dfx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://nemo:eatsalot@localhost/catrecipes_db\n",
      "postgresql://nemo:eatsalot@localhost/veggie_db\n"
     ]
    }
   ],
   "source": [
    "tags = ['vegetarian', 'vegan']\n",
    "veggiesQuantity = {'fennel':1, 'kohlrabi':1, 'carrot':5, 'tomato':10, 'broccoli':2}\n",
    "veggies = veggiesQuantity.keys()\n",
    "df = getRecipesFormTags(tags)\n",
    "recipesListFromTags = list(set(sum([recipe_id for recipe_id in df.recipe_ids.str.split(',')], [])))\n",
    "vDf = getRecipesForVeggies(veggies)\n",
    "recipesListFromVeggies = list(set(sum([recipe_id for recipe_id in vDf.recipes.str.split(',')], [])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sidesDf = getRecipesForTags(['side'])\n",
    "#sidesListFromTags = list(set(sum([recipe_id for recipe_id in sidesDf.recipe_ids.str.split(',')], [])))\n",
    "#len(recipesListFromTags), len(recipesListFromVeggies)\n",
    "#selectedRecipesList2 = list(set(recipesListFromTags)&set(recipesListFromVeggies))\n",
    "#len(selectedRecipesList2)\n",
    "#selectedRecipesList = list(set(selectedRecipesList2)-set(sidesListFromTags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n"
     ]
    }
   ],
   "source": [
    "selectedRecipesList = list(set(recipesListFromTags)&set(recipesListFromVeggies))\n",
    "print len(selectedRecipesList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://nemo:eatsalot@localhost/recipe_ingredients_db\n",
      "['tomato', 'fennel', 'carrot', 'broccoli', 'kohlrabi']\n",
      "318\n"
     ]
    }
   ],
   "source": [
    "\n",
    "selDf = getRecipeIngredientsForVeggies(selectedRecipesList, veggies)\n",
    "selDf = selDf.fillna(0)\n",
    "print veggies\n",
    "\n",
    "print len(selDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomato\n",
      "fennel\n",
      "carrot\n",
      "broccoli\n",
      "kohlrabi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selDf.head()\n",
    "for veggie in veggies:\n",
    "    print veggie\n",
    "    selDf[veggie] = selDf[veggie].astype(float)\n",
    "    selDf[veggie] = (selDf[veggie])/np.max(selDf[veggie])\n",
    "vDf['norm_shelf_life'] = vDf['shelf_life']/np.max(vDf['shelf_life'])\n",
    "len(selDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-87d3594c321e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdiffinQuantity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mveggie\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mselDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_servings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_of_servings\u001b[0m \u001b[0;34m-\u001b[0m                                 \u001b[0mveggiesQuantity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mveggie\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m                       \u001b[0mselDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mveggiesQuantity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mselDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_servings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_of_servings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mveggie\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mveggie\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_shelf_life\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mproduct\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mdiffinQuantity\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mselDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mselDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "selDf['score'] = 0.0\n",
    "num_of_servings = 4\n",
    "## regularize variables \n",
    "for veggie in veggiesQuantity.keys():\n",
    "    diffinQuantity = 1/(np.abs((selDf[veggie].astype(float)/selDf['num_servings'].astype(float)*num_of_servings - \\\n",
    "                                veggiesQuantity[veggie]*1.0 ))+\\\n",
    "                       selDf[veggiesQuantity.keys()].mean(axis=1)/selDf['num_servings'].astype(float)*num_of_servings)\n",
    "    weight = 1/(vDf[vDf.veggie == veggie].norm_shelf_life)\n",
    "    product =  diffinQuantity*list(set(weight))[0]\n",
    "    selDf['score'] = selDf['score'] + product\n",
    "selDf['score'] = selDf['score']\n",
    "selDf['recipe_name'] = selDf['index'].str.split('/').apply(lambda x: ' '.join(x[4].split('-')[:-1]).title())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "veggie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selDf['num_servings'].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Plot 1 (recipes sorted by score)\n",
    "#pd.options.display.mpl_style = 'default'\n",
    "plt.figure()\n",
    "plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "selDf = selDf.sort_values('score', ascending=False).head(6)\n",
    "#df[['name', 'rating']].head(50).plot(kind = 'barh', figsize = [12,20])\n",
    "plt.barh(range(len(selDf)), selDf['score'], height=0.8,align=\"center\", edgecolor=\"none\")\n",
    "plt.yticks(range(len(selDf)), selDf.recipe_name)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('Recipe Score')\n",
    "plt.title('Recipe Scores')\n",
    "plt.axis('tight')\n",
    "plt.savefig('recipeScores.png', format='png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meltedDf = pd.melt(selDf, id_vars=['score', 'recipe_name'], value_vars=veggies)\n",
    "# Map vegetables to expiry date\n",
    "shelfLife = dict(zip(veggies, [list(set(vDf[vDf.veggie == veggie].shelf_life))[0] for veggie in veggies]))\n",
    "recipeNames = list(set(selDf.recipe_name))\n",
    "recipeScores = dict(zip(recipeNames, [selDf[selDf['recipe_name']==rname].score for rname in recipeNames]))\n",
    "meltedDf['shelf_life'] = meltedDf['variable'].map(shelfLife)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "veggies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meltedDf = meltedDf.rename(columns = {'variable':'veggie'})\n",
    "meltedDf['new_score_log'] = np.log(meltedDf['score'])* meltedDf['value'].astype(float)\n",
    "meltedDf['new_score'] = meltedDf['score']* meltedDf['value'].astype(float)\n",
    "for veggie in veggies:\n",
    "    print veggie, np.mean(meltedDf[meltedDf['veggie']== veggie].new_score), np.mean(meltedDf[meltedDf['veggie']== veggie].shelf_life),\\\n",
    "    np.std(meltedDf[meltedDf['veggie']== veggie].shelf_life), np.mean(meltedDf[meltedDf['veggie']== veggie].value.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create an histogram of the plot\n",
    "## Hexagonal plot\n",
    "plt.figure()\n",
    "plt.rcParams['figure.figsize'] = (10.0, 7.0)\n",
    "fig = meltedDf.plot(kind='hexbin', x='new_score', y='shelf_life', gridsize=10,  sharex=False, cmap = 'YlGnBu', fontsize=14)\n",
    "#fig = selectedUserDf.plot(kind='scatter', x='myBookId', y='Rating_x', c = 'userID')#, gridsize=100,  sharex=False, fontsize=14)\n",
    "\n",
    "plt.xlabel('Book Title ID', fontsize=14)\n",
    "plt.ylabel('User IDs', fontsize=14)\n",
    "plt.title('2-D Histogram representing the number of times a book has been rated by different users')\n",
    "plt.setp(fig, 'visible', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imageDf = getRecipeImageURLs(selDf)\n",
    "\n",
    "selDf['id'] = selDf['index']\n",
    "aDf = pd.merge(selDf, imageDf, on='id', how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aDf['index'].apply(lambda x: \"http://www.epicurious.com\"+str(x))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Communicate with SQL database and get recipe tags and description\n",
    "def getRecipeDescriptionAndTags():\n",
    "    username = 'nemo'\n",
    "    pswd = 'eatsalot'\n",
    "    dbname = 'recipes_db'\n",
    "    engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "    print engine.url\n",
    "    \n",
    "    con = None\n",
    "    con = psycopg2.connect(database = dbname, user = username, host='localhost', password=pswd)\n",
    "    cur = con.cursor()\n",
    "    cur.execute(\"SELECT description,tags,id,name, review_text FROM recipes_db;\")\n",
    "    dfx = pd.DataFrame(cur.fetchall(), columns = ['description_one', 'tags', 'id', 'recipe_name', 'review_text'])\n",
    "    return dfx  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lemmatizePhrase(phrase):\n",
    "    \"\"\" \n",
    "    Lemmatize each word in a pharse\n",
    "    \"\"\"\n",
    "    words = phrase.lower().split()\n",
    "    return ' '.join(str(WordNetLemmatizer().lemmatize(word.encode('utf-8'))) for word in words)\n",
    "\n",
    "def doNLPStuff(selDfx, keywords):\n",
    "    keywordList = lemmatizePhrase(keywords)\n",
    "    tDf = preprocess()\n",
    "    dictionary = corpora.Dictionary.load('/tmp/descri.dict')\n",
    "    #corpus_tfidf = corpora.MmCorpus('/tmp/corpus_tfidf.mm')\n",
    "    index = similarities.MatrixSimilarity.load('/tmp/tfidf_lsi_similarities.index')\n",
    "    lsi = models.LsiModel.load('/tmp/model.lsi')\n",
    "    \n",
    "    b=index[lsi[dictionary.doc2bow(keywordList.split())]]\n",
    "    tDf['LSI_sim'] = b\n",
    "    selDfx2 = pd.merge(selDfx, tDf[['id', 'LSI_sim']], on=['id'], how='inner')\n",
    "    selDfx3 = selDfx2[selDfx2['LSI_sim']>0.1]\n",
    "    return selDfx3\n",
    "\n",
    "def preprocess():\n",
    "    tDf = getRecipeDescriptionAndTags();\n",
    "    tDf['tags'] = tDf.tags.str.rsplit(\",\")\n",
    "    tempTags = list(itertools.chain.from_iterable(list(tDf.tags)))\n",
    "    tags = list(set(tempTags))\n",
    "    return tDf\n",
    "\n",
    "keywords = 'salad'\n",
    "selDf['id'] = selDf['index']\n",
    "selDf2 = pd.merge(selDf, imageDf, on='id', how='outer')\n",
    "selDf2['index'] = selDf2['index'].apply(lambda x: \"http://www.epicurious.com\"+str(x))\n",
    "selDf3 = doNLPStuff(selDf2, keywords)\n",
    "selDf3.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#meltedDf['log_newscore'] = np.log(meltedDf['new_score'])\n",
    "p1 = BoxPlot(meltedDf[['new_score', 'veggie']], values='new_score', label=CatAttr(columns=['veggie'], sort=False),\n",
    "            title=\"Variation in scores among ingredients\")\n",
    "\n",
    "output_file(\"boxplot.html\")\n",
    "show(p1)\n",
    "#meltedDf['log_newscore'] = np.log(meltedDf['new_score'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meltedDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len([None, None])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Try running NLP on this shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tDf = getRecipeDescriptionAndTags();\n",
    "tDf['tags'] = tDf.tags.str.rsplit(\",\")\n",
    "tempTags = list(itertools.chain.from_iterable(list(tDf.tags)))\n",
    "tags = list(set(tempTags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tDf['description'] = tDf['description_one'].map(str)+tDf['recipe_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create a corpus with words from description\n",
    "# Remove shitty unicode characters\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc2\\x97', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xb3', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xa8', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xa9', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xe2\\x80\\x99', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xb1', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xe2\\x80\\x89', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xe2\\x80\\x94', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc2\\x94', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc2\\x93', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc2\\x92', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xe2\\x80\\xa6', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xaa', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xe2\\x80\\x9c', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xe2\\x80\\x93', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xe2\\x80\\x9d', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xe2\\x80\\xa2', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xa1', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xa2', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xb9', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc2\\xb7', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc2\\xb4', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xb4', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xa4', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xae', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xe2\\x80\\xa8', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xb3', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xa0', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc2\\xb0', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xaf', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xad', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc2\\x96', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xb8', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xa7', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xe2\\x80\\xa7', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc2\\x95', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xbb', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xab', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xb6', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xb1', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xe2\\x84\\xa2', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xbc', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc2\\xbf', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xe2\\x80\\x91', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\x81', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc2\\x85', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc2\\xa9', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xb6', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xba', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xaa', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xb2', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\x87', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\xbe', ' ')\n",
    "tDf['description'] = tDf['description'].str.replace('\\xc3\\x89', 'e')\n",
    "tDf['description'] = tDf['description'].str.replace('\\x89', ' ')\n",
    "tDf['description'] = tDf['description'].apply(lambda x: unicode(x, errors = 'ignore'))\n",
    "#tDf['description'] = tDf['description'].str.encode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do NLP preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## tokenize words. Basically split words from sentences. To remove punctuations I used RegexpTokenizer!\n",
    "#tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer = RegexpTokenizer(r'[a-z]\\w+')\n",
    "tDf['description'] = tDf['description'].str.lower()\n",
    "tDf['tokens'] = tDf['description'].apply(lambda x:tokenizer.tokenize(x))\n",
    "\n",
    "## Now remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tDf['tokens'] = tDf['tokens'].apply(lambda x: [word.lower() for word in x if not word.lower() in stop_words])\n",
    "\n",
    "## Lemma or stem? Let me lemma for the sake of doing it properly\n",
    "## lemma differentiates between make and making, it is irrelavant!\n",
    "ps = PorterStemmer()\n",
    "tDf['tokens'] = tDf['tokens'].apply(lambda x: [WordNetLemmatizer().lemmatize(str(word)) for word in x])\n",
    "tDf['tokens'] = tDf['tokens'].apply(lambda x:tokenizer.tokenize(str(x)))\n",
    "#tDf['tokens'] = tDf['tokens'].apply(lambda x: [ps.stem(str(word)) for word in x])\n",
    "#tDf['tokens'] = tDf['tokens'].apply(lambda x:tokenizer.tokenize(str(x)))\n",
    "#df['tokens'] = df['tokens'].apply(lambda x: [ps.stem(str(word)) for word in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LSI and LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Create a corpora where each word is assigned a unique ID\n",
    "dictionary = corpora.Dictionary(list(tDf['tokens']))\n",
    "dictionary.save('/tmp/descri.dict')\n",
    "print dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remove words that occur less than 5 times and have len < 2\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq < 1 ]\n",
    "#wordlength_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if len(dictionary[tokenid]) < 2 ]\n",
    "dictionary.filter_tokens(once_ids )\n",
    "dictionary.compactify()\n",
    "dictionary.save('/tmp/descri.dict')\n",
    "print dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The function doc2bow() simply counts the number of occurences of each distinct word, converts the word to its integer word id and returns the result as a sparse vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in tDf['tokens']]\n",
    "corpora.MmCorpus.serialize('/tmp/descri.mm', corpus)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the most common words, try to filter out useless ones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "wordFreq = defaultdict(list)\n",
    "for tokenid, docfreq in dictionary.dfs.iteritems():\n",
    "    #print dictionary[tokenid], docfreq\n",
    "    wordFreq[dictionary[tokenid]] = docfreq\n",
    "wordFreqDf = pd.DataFrame(wordFreq.items(), columns = ['word', 'freq'])\n",
    "wordFreqDf = wordFreqDf.sort_values('freq', ascending=False)\n",
    "print wordFreqDf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf [Term Frequency-inverse document frequency weighting]\n",
    "words that occur frequently within a document but not frequently within the corpus receive a higher weighting as these words are assumed to contain more meaning in relation to the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus, normalize=True)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "corpora.MmCorpus.serialize('/tmp/corpus_tfidf.mm', corpus_tfidf)\n",
    "index = similarities.MatrixSimilarity(corpus_tfidf )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try LSI and LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics = 30)\n",
    "index = similarities.MatrixSimilarity(lda[corpus_tfidf])\n",
    "lda.save('/tmp/model.lda')\n",
    "lda = models.LdaModel.load('/tmp/model.lda')\n",
    "index.save('/tmp/tfidf_lda_similarities.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=index[lda[dictionary.doc2bow(['asia'])]]\n",
    "a_index = np.argsort(a)\n",
    "print tDf['recipe_name'][a_index[-10:]]\n",
    "print a[a_index[-10:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary)\n",
    "index_lsi = similarities.MatrixSimilarity(lsi[corpus_tfidf])\n",
    "lsi.save('/tmp/model.lsi')\n",
    "lsi = models.LsiModel.load('/tmp/model.lsi')\n",
    "index_lsi.save('/tmp/tfidf_lsi_similarities.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b=index_lsi[lsi[dictionary.doc2bow(['pasta'])]]\n",
    "b_index = np.argsort(b)\n",
    "print tDf['recipe_name'][b_index[-10:]]\n",
    "print b[b_index[-10:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b=index_lsi[lsi[dictionary.doc2bow(['soup'])]]\n",
    "bgreat_index = b>0.3\n",
    "print tDf['recipe_name'][bgreat_index]\n",
    "print b[bgreat_index]\n",
    "print len(b[bgreat_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selDf.reset_index().to_json(orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the performance of LSI or LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tDf['LSI_sim'] = b\n",
    "tDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "selDf2 = pd.merge(selDf, tDf[['id', 'LSI_sim']], on=['id'])\n",
    "selDf2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selDf2[selDf2['LSI_sim']>0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soupsDf = getRecipesForTags(['asia'])\n",
    "soupsListFromTags = list(set(sum([recipe_id for recipe_id in soupsDf.recipe_ids.str.split(',')], [])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(soupsListFromTags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tDf['salad_cat'] = 0\n",
    "tDf[tDf['id'].isin(soupsListFromTags)].salad_cat = 1\n",
    "tDf['salad_word_match'] = 0\n",
    "a = tDf.recipe_name.str.lower().apply(lambda x: unicode(x, errors = 'ignore'))\n",
    "tDf[a.str.contains('salad')].salad_word_match = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tDf[tDf['id'].isin(soupsListFromTags)] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tDf[tDf['id'].isin(soupsListFromTags)].salad_cat = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doNLPStuff(selDf, tDf, keywords):\n",
    "    dictionary = corpora.Dictionary.load('/tmp/descri.dict')\n",
    "    corpus = corpora.MmCorpus('/tmp/descri.mm')\n",
    "    index = similarities.MatrixSimilarity.load('/tmp/tfidf_lsi_similarities.index')\n",
    "    lsi = models.LsiModel.load('/tmp/model.lsi')\n",
    "    \n",
    "    b=index[lsi[dictionary.doc2bow(keywords)]]\n",
    "    tDf['LSI_sim'] = b\n",
    "    selDf2 = pd.merge(selDf, tDf[['id', 'LSI_sim']], on=['id'])\n",
    "    selDf3 = selDf2[selDf2['LSI_sim']>0.1]\n",
    "    return selDf3\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
