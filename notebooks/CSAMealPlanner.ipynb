{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan your meals from CSA shit!\n",
    "Recipe data source: Epicurious.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Import all libraries\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import urllib2, time, pickle\n",
    "\n",
    "# SQL packages\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I am scraping for the recipes using vegetables typically grown in my old CSA, because they have that list and also because they have some sample boxes that I can test my algorithm on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testCsaUrl = \"http://driftlessorganics.com/csa-2/recipes/\"\n",
    "page = urllib2.urlopen(testCsaUrl)\n",
    "vegetableSoup = BeautifulSoup(page, \"html.parser\")\n",
    "veggiesHTML = vegetableSoup.findAll('span', style=\"color: #339966;\")\n",
    "\n",
    "vegetables = []\n",
    "for veggie in veggiesHTML:\n",
    "    if veggie.a:\n",
    "        vegetables.append(veggie.a.get_text().lower())\n",
    " \n",
    "### Great I have a list of vegetables! Now to clean them up! Like split up Cauliflower and Romanesco!\n",
    "vegetablesSplit = [vegetable.split('&',1) for vegetable in vegetables]\n",
    "veggieList = sum(vegetablesSplit, [])\n",
    "#veggiesList = veggieList[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gm\n",
    "\n",
    "\n",
    "foodSpoilURL = \"http://ohmyveggies.com/how-to-prevent-food-spoilage-with-careful-meal-planning/\"\n",
    "#page = urllib2.urlopen(foodSpoilURL)\n",
    "hdr = {'User-Agent':'Mozilla/5.0'}\n",
    "req = urllib2.Request(foodSpoilURL,headers=hdr)\n",
    "page = urllib2.urlopen(req)\n",
    "foodSpoilSoup = BeautifulSoup(page, \"html.parser\")\n",
    "foodSpoilHtml = foodSpoilSoup.findAll(\"p\")\n",
    "\n",
    "foodSpoilsinDate12Days = foodSpoilHtml[7].get_text().split()\n",
    "foodSpoilsin35Days = foodSpoilHtml[9].get_text().split()\n",
    "foodSpoilsin67Days = foodSpoilHtml[11].get_text().split()\n",
    "foodSpoilsGreatWeek = foodSpoilHtml[13].get_text().split()\n",
    "\n",
    "#foodSpoilDates = [foodSpoilsinDate12Days, foodSpoilsin35Days, foodSpoilsin67Days, foodSpoilsGreatWeek]\n",
    "\n",
    "foodSpoilsinDate12Days_lemma = [lemmatizePhrase(str(veggie).lower()) for veggie in foodSpoilsinDate12Days]   \n",
    "foodSpoilsinDate35Days_lemma = [lemmatizePhrase(str(veggie).lower()) for veggie in foodSpoilsin35Days]  \n",
    "foodSpoilsinDate67Days_lemma = [lemmatizePhrase(str(veggie).lower()) for veggie in foodSpoilsin67Days] \n",
    "foodSpoilsGreatWeek_lemma = [lemmatizePhrase(str(veggie).lower()) for veggie in foodSpoilsGreatWeek] \n",
    "foodSpoilsGreatWeek_lemma = foodSpoilsGreatWeek_lemma[:13] + foodSpoilsGreatWeek_lemma[22:]\n",
    "foodSpoilsGreatWeek_lemma.append('dill')\n",
    "foodSpoilsinDate35Days_lemma = foodSpoilsinDate35Days_lemma[:2]+foodSpoilsinDate35Days_lemma[14:]\n",
    "foodSpoilsinDate35Days_lemma.append('basil')\n",
    "\n",
    "veggies1 = foodSpoilsinDate12Days_lemma + foodSpoilsinDate35Days_lemma + \\\n",
    "foodSpoilsinDate67Days_lemma + foodSpoilsGreatWeek_lemma\n",
    "veggies1.remove('and')\n",
    "veggies1.remove('other')\n",
    "veggies1.remove('hot')\n",
    "veggies2 = list(set(veggies1)-set(veggieList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(set(veggieList)-set(veggies1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap epicurious.com for recipes for every vegetable in my CSA list!\n",
    "because they are free, I see they have tags (gluten-free/kid friendly etc), reviews, ratings and sort recipes by ingredients, calorie info\n",
    "\n",
    "*** Will not scrap data from a recipe without images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome(\"chromedriver\")\n",
    "def getRecipes(df, veggiesList, pageNumber, visiting = None):\n",
    "    if visiting is None:\n",
    "        visiting = set()\n",
    "        if len(veggiesList) == 0:\n",
    "            return df\n",
    "        else:\n",
    "            veggie = veggiesList.pop()\n",
    "            visiting.add(veggie)\n",
    "            print veggie, pageNumber\n",
    "        \n",
    "    recipes = defaultdict(list)\n",
    "    \n",
    "    pageSize = 30\n",
    "    resultOffset = pageNumber*pageSize + 1\n",
    "    url = \\\n",
    "    \"http://www.epicurious.com/tools/searchresults?search=%s&sort=1&type=simple&pageNumber=%s&pageSize=%s&resultOffset=%s\"%\\\n",
    "    (list(visiting)[0], pageNumber, pageSize, resultOffset)\n",
    "    browser.get(url)\n",
    "    time.sleep(2.0)\n",
    "    recipeListPage = browser.page_source\n",
    "    soup2 = BeautifulSoup(recipeListPage, \"html.parser\")\n",
    "    \n",
    "    \n",
    "    #allRecipes.append(soup2.find('div', class_='sr_rows clearfix firstResult').a.get('href'))\n",
    "    if soup2.find('div', class_='sr_rows clearfix firstResult') is None:\n",
    "        return getRecipes(df, veggiesList, 1, visiting = None)\n",
    "        \n",
    "    recipes['recipe'].append(soup2.find('div', class_='sr_rows clearfix firstResult').a.get('href'))\n",
    "    recipes['searchIngredient'].append(list(visiting)[0])\n",
    "    recipesDf = pd.DataFrame.from_dict(recipes)\n",
    "    df = pd.concat([df, recipesDf])\n",
    "    \n",
    "    a=soup2.findAll('div', class_='sr_rows clearfix ')\n",
    "    count = 0\n",
    "    for recipeList in a:\n",
    "        count += 1\n",
    "        if recipeList.find('img').get('src') != \"/css/i/recipe-img-icon.png\":\n",
    "            #allRecipes.append(recipeList.a.get('href'))\n",
    "            recipes['recipe'].append(recipeList.a.get('href'))\n",
    "            recipes['searchIngredient'].append(list(visiting)[0])\n",
    "            recipesDf = pd.DataFrame.from_dict(recipes)\n",
    "            df = pd.concat([df, recipesDf])\n",
    "    \n",
    "    if (count < 28) | (pageNumber > 100):\n",
    "        return getRecipes(df, veggiesList, 1, visiting = None)\n",
    "    else:\n",
    "        return getRecipes(df, veggiesList, pageNumber+1, visiting)\n",
    "  \n",
    "df = pd.DataFrame(columns = ('recipe', 'searchIngredient'))\n",
    "allRecipes = []\n",
    "initDf = getRecipes(df, ['vegan'], 1)\n",
    "initDf.to_pickle('recipeList4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Had saved recipes per \n",
    "df = pd.DataFrame(columns = ('recipe', 'searchIngredient'))\n",
    "allRecipes = []\n",
    "#initDf = getRecipes(df, veggies1, 1)\n",
    "#initDf.to_pickle('recipeList3')\n",
    "df1 = pd.read_pickle('/Volumes/LittleOne/Insight/RecipeProject/recipeList')\n",
    "df2 = pd.read_pickle('/Volumes/LittleOne/Insight/RecipeProject/recipeList2')\n",
    "df3 = pd.read_pickle('/Volumes/LittleOne/Insight/RecipeProject/recipeList3')\n",
    "df4 = pd.read_pickle('/Volumes/LittleOne/Insight/RecipeProject/recipeList6')\n",
    "df = pd.concat([df1, df2, df3, df4]).drop_duplicates('recipe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome(\"chromedriver\")\n",
    "rDf = pd.DataFrame(columns = ('id', 'name', 'tags', 'description', 'imageURL', 'rating', 'willPrepareAgainRating',\\\n",
    "                                  'reviewCount', 'numberOfServings', 'activeTime', 'totalTime', 'numberOfCalories',\\\n",
    "                                  'ingredientsList', 'reviewText'))\n",
    "recipes = defaultdict(list)\n",
    "recipeIDs = list(set(df.recipe))\n",
    "oldRecipesDf1 =  pd.read_pickle('/Volumes/LittleOne/Insight/RecipeProject/recipesDb1')\n",
    "oldRecipesDf2 =  pd.read_pickle('/Volumes/LittleOne/Insight/RecipeProject/recipesDb2')\n",
    "oldRecipesDf3 =  pd.read_pickle('/Volumes/LittleOne/Insight/RecipeProject/recipesDb3')\n",
    "oldRecipesDf4 =  pd.read_pickle('/Volumes/LittleOne/Insight/RecipeProject/recipesDb4')\n",
    "oldRecipesDf5 =  pd.read_pickle('/Volumes/LittleOne/Insight/RecipeProject/recipesDb5')\n",
    "oldRecipesDf6 =  pd.read_pickle('/Volumes/LittleOne/Insight/RecipeProject/recipesDb6')\n",
    "oldRecipesDf = pd.concat([oldRecipesDf1, oldRecipesDf2, oldRecipesDf3, oldRecipesDf4, oldRecipesDf5, oldRecipesDf6])\n",
    "recipeIDs = list(set(df.recipe)-set(oldRecipesDf.id)) \n",
    "\n",
    "for recipeID in recipeIDs:\n",
    "    url = \"http://www.epicurious.com\"+recipeID\n",
    "    browser.get(url)\n",
    "    \n",
    "    recipePage = browser.page_source\n",
    "    soup = BeautifulSoup(recipePage, \"html.parser\")\n",
    "    if (soup.find(itemprop = \"ratingValue\") is not None) & (soup.find(itemprop = \"keywords\") is not None):\n",
    "        recipes['id'].append(recipeID)\n",
    "        recipes['name'].append(soup.find(property = \"og:title\").get('content'))\n",
    "        if soup.find(itemprop = \"keywords\") is None:\n",
    "            recipes['tags'].append(' ')\n",
    "        else:\n",
    "            recipes['tags'].append(soup.find(itemprop = \"keywords\").get('content'))\n",
    "        if soup.find(property = \"og:description\") is None:\n",
    "            recipes['description'].append(' ')\n",
    "        else:\n",
    "            recipes['description'].append(soup.find(property = \"og:description\").get('content'))\n",
    "        recipes['imageURL'].append(soup.find(property = \"og:image\").get('content'))\n",
    "    \n",
    "    \n",
    "        recipes['rating'].append(soup.find(itemprop = \"ratingValue\").get('content'))\n",
    "        recipes['willPrepareAgainRating'].append(soup.find('div', class_=\"prepare-again-rating\").span.get_text())\n",
    "        recipes['reviewCount'].append(soup.find('span', class_=\"reviews-count\").get_text())\n",
    "\n",
    "        if soup.find('div', class_ = \"recipe-summary\") is None:\n",
    "            recipes['numberOfServings'].append(' ')\n",
    "        else:\n",
    "            recipes['numberOfServings'].append(soup.find('div', class_ = \"recipe-summary\").dd.get_text())\n",
    "\n",
    "        if ((soup.find('dd', class_ = \"active-time\")) is None) |  (soup.find('dd', class_ = \"total-time\") is None):\n",
    "            recipes['activeTime'].append(' ')\n",
    "            recipes['totalTime'].append(' ')\n",
    "        else:\n",
    "            recipes['activeTime'].append(soup.find('dd', class_ = \"active-time\").get_text())\n",
    "            recipes['totalTime'].append(soup.find('dd', class_ = \"total-time\").get_text())\n",
    "\n",
    "        if soup.find(itemprop = \"calories\") is None:\n",
    "            recipes['numberOfCalories'].append(' ')\n",
    "        else:\n",
    "            recipes['numberOfCalories'].append(soup.find(itemprop = \"calories\").get_text())\n",
    "\n",
    "        # Create a list of ingredients\n",
    "        ingredientsList = [] \n",
    "        ingredients = soup.findAll(itemprop = \"ingredients\")\n",
    "        for ingredient in ingredients:\n",
    "            ingredientsList.append(ingredient.get_text())\n",
    "        recipes['ingredientsList'].append(ingredientsList)\n",
    "\n",
    "        # Create a list of reviews\n",
    "        reviewText = []\n",
    "        reviews = soup.findAll('div', class_ = \"review-text\")\n",
    "        for review in reviews:\n",
    "            reviewText.append(review.p.get_text())\n",
    "        recipes['reviewText'].append(reviewText)\n",
    "\n",
    "recipesDf = pd.DataFrame.from_dict(recipes)\n",
    "recipesDf.to_pickle('recipesDb6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recipesDf = pd.DataFrame.from_dict(recipes)\n",
    "recipesDf.to_pickle('recipesDb6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oldRecipesDf1 =  pd.read_pickle('/Volumes/LittleOne/Insight/RecipeProject/recipesDb1')\n",
    "oldRecipesDf2 =  pd.read_pickle('/Volumes/LittleOne/Insight/RecipeProject/recipesDb2')\n",
    "oldRecipesDf3 =  pd.read_pickle('/Volumes/LittleOne/Insight/RecipeProject/recipesDb3')\n",
    "oldRecipesDf4 =  pd.read_pickle('/Volumes/LittleOne/Insight/RecipeProject/recipesDb4')\n",
    "oldRecipesDf5 =  pd.read_pickle('/Volumes/LittleOne/Insight/RecipeProject/recipesDb5')\n",
    "oldRecipesDf6 =  pd.read_pickle('/Volumes/LittleOne/Insight/RecipeProject/recipesDb6')\n",
    "oldRecipesDf = pd.concat([oldRecipesDf1, oldRecipesDf2, oldRecipesDf3, oldRecipesDf4, oldRecipesDf5, oldRecipesDf6])\n",
    "recipeIDs = list(set(df.recipe)-set(oldRecipesDf.id))\n",
    "len(recipeIDs), len(oldRecipesDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recipesDf = pd.DataFrame.from_dict(recipes)\n",
    "recipesDf.to_pickle('recipesDb5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recipesDf = pd.DataFrame.from_dict(recipes)\n",
    "recipesDf.to_pickle('recipesDb2')\n",
    "oldRecipesDf.to_pickle('recipesDb1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get food spoilage dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### get food spoilage dates\n",
    "foodSpoilURL = \"http://ohmyveggies.com/how-to-prevent-food-spoilage-with-careful-meal-planning/\"\n",
    "#page = urllib2.urlopen(foodSpoilURL)\n",
    "hdr = {'User-Agent':'Mozilla/5.0'}\n",
    "req = urllib2.Request(foodSpoilURL,headers=hdr)\n",
    "page = urllib2.urlopen(req)\n",
    "foodSpoilSoup = BeautifulSoup(page, \"html.parser\")\n",
    "foodSpoilHtml = foodSpoilSoup.findAll(\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "foodSpoilsinDate12Days = foodSpoilHtml[7].get_text().split()\n",
    "foodSpoilsin35Days = foodSpoilHtml[9].get_text().split()\n",
    "foodSpoilsin67Days = foodSpoilHtml[11].get_text().split()\n",
    "foodSpoilsGreatWeek = foodSpoilHtml[13].get_text().split()\n",
    "foodSpoilDates = [foodSpoilsinDate12Days, foodSpoilsin35Days, foodSpoilsin67Days, foodSpoilsGreatWeek]\n",
    "with open(\"foodspoils.txt\", \"wb\") as fo:\n",
    "    pickle.dump(foodSpoilDates, fo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SQL database for my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dbname = 'recipes_db'\n",
    "username = 'nemo'\n",
    "pswd = 'eatsalot'\n",
    "\n",
    "engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "print engine.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## create a database (if it doesn't exist)\n",
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)\n",
    "print(database_exists(engine.url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviewText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Parse a list of vegetables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def lemmatizePhrase(phrase):\n",
    "    \"\"\" \n",
    "    Lemmatize each word in a pharse\n",
    "    \"\"\"\n",
    "    words = phrase.lower().split()\n",
    "    return ' '.join(WordNetLemmatizer().lemmatize(word) for word in words)\n",
    "\n",
    "ingredientsURL = \"https://en.m.wikibooks.org/wiki/Cookbook:Ingredients\"\n",
    "page = urllib2.urlopen(ingredientsURL)\n",
    "ingredientsSoup = BeautifulSoup(page, \"html.parser\")\n",
    "ingredientsHtml = ingredientsSoup.findAll(\"li\")\n",
    "ingredients = []\n",
    "\n",
    "for ingredient in ingredientsHtml:\n",
    "    if ingredient.a is not None:\n",
    "        ingredients.append(lemmatizePhrase(ingredient.a.get_text()))\n",
    "\n",
    "ingredients = list(set(ingredients))\n",
    "ingredients.remove('cc by-sa 3.0')\n",
    "ingredients.remove('')\n",
    "ingredients.remove('edit')\n",
    "ingredients.remove('desktop')\n",
    "ingredients.remove('privacy')\n",
    "ingredients.remove('soda, baking')\n",
    "veggies = []\n",
    "for veggie in veggieList:\n",
    "    ingredients.append(lemmatizePhrase(veggie.lower()))\n",
    "    veggies.append(lemmatizePhrase(veggie.lower()))\n",
    "ingredients = list(set(ingredients))\n",
    "with open(\"ingredients.txt\", \"wb\") as fo:\n",
    "    pickle.dump(ingredients, fo)\n",
    "with open(\"veggies.txt\", \"wb\") as fo:\n",
    "    pickle.dump(veggies, fo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse a list of food adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "foodAdjectivesURL = \"https://foxhugh.com/word-lists/list-of-food-adjectives/\"\n",
    "page = urllib2.urlopen(foodAdjectivesURL)\n",
    "foodAdjectivesSoup = BeautifulSoup(page, \"html.parser\")\n",
    "foodAdjectivesHtml = foodAdjectivesSoup.findAll(\"p\")\n",
    "adjectives = []\n",
    "for adjective in foodAdjectivesHtml:\n",
    "    adjectives.append(lemmatizePhrase(adjective.get_text()))\n",
    "adjectives = adjectives[1:302]\n",
    "with open(\"adjectives.txt\", \"wb\") as fo:\n",
    "    pickle.dump(adjectives, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for veggie in veggieList:\n",
    "    ingredients.append(lemmatizePhrase(veggie.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(list(set(ingredients)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
